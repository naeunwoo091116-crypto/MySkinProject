"""
GPU ì„œë²„ìš© AI ì¶”ë¡  API
ì´ íŒŒì¼ì„ GPU ì„œë²„ì— ë°°í¬í•˜ì—¬ AI ëª¨ë¸ ì¶”ë¡ ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
import io
from PIL import Image
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.ai_service import AIService
from services.image_service import ImageService
from services.chatbot_service import get_chatbot_service
from core.logger import setup_logger

app = Flask(__name__)
CORS(app)  # ëª¨ë“  ë„ë©”ì¸ì—ì„œ API í˜¸ì¶œ í—ˆìš©

logger = setup_logger(__name__)

# AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (GPU ì‚¬ìš©)
ai_service = AIService()
image_service = ImageService()

# ì±—ë´‡ ì„œë¹„ìŠ¤ (Lazy loading - ì²« ìš”ì²­ ì‹œ ë¡œë“œ)
chatbot_service = None

@app.route('/')
def home():
    """Health check"""
    return jsonify({
        "status": "ok",
        "service": "MySkin AI Inference API",
        "gpu_available": torch.cuda.is_available(),
        "device": str(ai_service.device)
    })

@app.route('/api/v1/inference', methods=['POST'])
def inference():
    """
    AI ëª¨ë¸ ì¶”ë¡  API

    Input (multipart/form-data):
        - file: ì–¼êµ´ ì´ë¯¸ì§€

    Output:
        - regions: ë¶€ìœ„ë³„ ë¶„ì„ ê²°ê³¼
        - overall_score: ì „ì²´ ì ìˆ˜
    """
    try:
        if 'file' not in request.files:
            return jsonify({"error": "No file provided"}), 400

        file = request.files['file']

        # 1. ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = file.read()
        pil_image = Image.open(io.BytesIO(image_bytes)).convert('RGB')

        # 2. ì´ë¯¸ì§€ ê²€ì¦ (ì–¼êµ´ ê°ì§€)
        is_valid, message = image_service.validate_image(pil_image)
        if not is_valid:
            return jsonify({
                "error": "invalid_image",
                "message": message
            }), 400

        # 3. AI ëª¨ë¸ ì¶”ë¡  (GPUì—ì„œ ì‹¤í–‰)
        predictions = ai_service.predict_all_regions(pil_image)

        logger.info(f"âœ… Inference completed on {ai_service.device}")

        return jsonify({
            "success": True,
            "predictions": predictions,
            "device": str(ai_service.device)
        })

    except ValueError as val_err:
        logger.error(f"Validation error: {val_err}")
        return jsonify({
            "error": "invalid_image",
            "message": str(val_err)
        }), 400

    except Exception as e:
        logger.error(f"Inference error: {e}", exc_info=True)
        return jsonify({
            "error": "inference_failed",
            "message": str(e)
        }), 500

@app.route('/api/v1/chatbot', methods=['POST'])
def chatbot():
    """
    LLM ì±—ë´‡ API (GPUì—ì„œ ì‹¤í–‰)

    Input (multipart/form-data):
        - message: ì‚¬ìš©ì ë©”ì‹œì§€
        - image: ì´ë¯¸ì§€ íŒŒì¼ (ì„ íƒì‚¬í•­)

    Output:
        - reply: ì±—ë´‡ ì‘ë‹µ
    """
    global chatbot_service

    try:
        # Lazy loading - ì²« ìš”ì²­ ì‹œ ëª¨ë¸ ë¡œë“œ
        if chatbot_service is None:
            logger.info("ğŸ¤– Loading chatbot model (first request)...")
            chatbot_service = get_chatbot_service()

        # ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        message = request.form.get('message', '')
        image_file = request.files.get('image')

        # JSON ìš”ì²­ ì²˜ë¦¬
        if not message and request.is_json:
            data = request.get_json()
            message = data.get('message', '')

        if not message and not image_file:
            return jsonify({"error": "No message or image provided"}), 400

        logger.info(f"ğŸ’¬ Chatbot request: {message[:50]}...")

        # ì±—ë´‡ ì‘ë‹µ ìƒì„± (GPUì—ì„œ ì‹¤í–‰)
        reply = chatbot_service.generate_response(message, image_file)

        logger.info(f"âœ… Chatbot response generated")

        return jsonify({
            "success": True,
            "reply": reply,
            "device": "cuda" if torch.cuda.is_available() else "cpu"
        })

    except Exception as e:
        logger.error(f"Chatbot error: {e}", exc_info=True)
        return jsonify({
            "error": "chatbot_failed",
            "message": str(e)
        }), 500

@app.route('/api/v1/health', methods=['GET'])
def health():
    """ì„œë²„ ìƒíƒœ ì²´í¬"""
    return jsonify({
        "status": "healthy",
        "gpu_available": torch.cuda.is_available(),
        "device": str(ai_service.device),
        "models_loaded": ai_service.models is not None,
        "chatbot_loaded": chatbot_service is not None
    })

if __name__ == '__main__':
    # GPU ì„œë²„ì—ì„œ ì‹¤í–‰
    # ì™¸ë¶€ ì ‘ê·¼ì„ ìœ„í•´ 0.0.0.0ìœ¼ë¡œ ë°”ì¸ë”©
    port = int(os.getenv('PORT', 8000))
    app.run(host='0.0.0.0', port=port, debug=False)
